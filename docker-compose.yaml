# docker-compose.yaml

services:
  # ----------------------------------------------------------------------
  # 1. Custom ETL Worker Service (Executes your Python scripts)
  # ----------------------------------------------------------------------
  etl_worker:
    build: 
      context: . 
      dockerfile: Dockerfile
    image: movie-etl-worker:latest
    container_name: movie_etl_worker
    volumes:
      # Mount DAGs so Airflow can see them
      - ./airflow_dags:/opt/airflow/dags
      # Mount the project root to /app/scripts in the worker (matches Dockerfile WORKDIR)
      - ./:/app/scripts                  
    env_file:
      - ./.env 
    environment:
      # CRITICAL: This allows the container to talk to your local PostgreSQL server
      - DB_HOST=host.docker.internal 
    # NO depends_on needed for the worker itself

  # ----------------------------------------------------------------------
  # 2. Airflow Webserver (UI)
  # ----------------------------------------------------------------------
  airflow_webserver:
    image: apache/airflow:2.8.1
    container_name: airflow_webserver
    restart: always
    ports:
      - "8080:8080" # Access Airflow UI at http://localhost:8080
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      # Airflow metadata database is SQLite for simplicity
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__WEBSERVER__SECRET_KEY=super-secret-key
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - airflow_data:/opt/airflow
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 30s
      retries: 5

  # ----------------------------------------------------------------------
  # 3. Airflow Scheduler
  # ----------------------------------------------------------------------
  airflow_scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow_scheduler
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - airflow_data:/opt/airflow
    command: scheduler
    depends_on:
      - airflow_webserver
      - etl_worker

volumes:
  airflow_data: